{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and Deep Neural Networks #\n",
    "\n",
    "We've collected some hyperparameters here to make experimentation easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "UNITS = 2 ** 11  # 2048\n",
    "ACTIVATION = 'relu'\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Training Configuration\n",
    "BATCH_SIZE_PER_REPLICA = 2 ** 11  # powers of 128 are best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few sections set up the TPU computation, data pipeline, and neural network model. \n",
    "\n",
    "# Setup #\n",
    "\n",
    "In addition to our imports, this section contains some code that will connect our notebook to the TPU and create a **distribution strategy**. Each TPU has eight computational cores acting independently. With a distribution strategy, we define how we want to divide up the work between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "# Detect and init the TPU\n",
    "try:  # detect TPUs\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()  # TPU detection\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError:  # detect GPUs\n",
    "    # default strategy that works on CPU and single GPU\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "# Plotting\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matplotlib defaults\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "\n",
    "# Data\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from tensorflow.io import FixedLenFeature\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that TensorFlow now detects eight accelerators. Using a TPU is a bit like using eight GPUs at once.\n",
    "\n",
    "# Load Data #\n",
    "\n",
    "The dataset has been encoded in a binary file format called *TFRecords*. These two functions will parse the TFRecords and build a TensorFlow `tf.data.Dataset` object that we can use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decoder(feature_description):\n",
    "    def decoder(example):\n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        features = tf.io.parse_tensor(example['features'], tf.float32)\n",
    "        features = tf.reshape(features, [28])\n",
    "        label = example['label']\n",
    "        return features, label\n",
    "    return decoder\n",
    "\n",
    "\n",
    "def load_dataset(filenames, decoder, ordered=False):\n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False\n",
    "    dataset = (\n",
    "        tf.data\n",
    "        .TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "        .with_options(ignore_order)\n",
    "        .map(decoder, AUTO)\n",
    "    )\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = int(11e6)\n",
    "validation_size = int(5e5)\n",
    "training_size = dataset_size - validation_size\n",
    "\n",
    "# For model.fit\n",
    "batch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "steps_per_epoch = training_size // batch_size\n",
    "validation_steps = validation_size // batch_size\n",
    "\n",
    "# For model.compile\n",
    "steps_per_execution = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'features': FixedLenFeature([], tf.string),\n",
    "    'label': FixedLenFeature([], tf.float32),\n",
    "}\n",
    "decoder = make_decoder(feature_description)\n",
    "\n",
    "data_dir = KaggleDatasets().get_gcs_path('higgs-boson')\n",
    "train_files = tf.io.gfile.glob(data_dir + '/training' + '/*.tfrecord')\n",
    "valid_files = tf.io.gfile.glob(data_dir + '/validation' + '/*.tfrecord')\n",
    "\n",
    "ds_train = load_dataset(train_files, decoder, ordered=False)\n",
    "ds_train = (\n",
    "    ds_train\n",
    "    .cache()\n",
    "    .repeat()\n",
    "    .shuffle(2 ** 19)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "ds_valid = load_dataset(valid_files, decoder, ordered=False)\n",
    "ds_valid = (\n",
    "    ds_valid\n",
    "    .batch(batch_size)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #\n",
    "\n",
    "Now that the data is ready, let's define the network. We're defining the deep branch of the network using Keras's *Functional API*, which is a bit more flexible that the `Sequential` method we used in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(units, activation, dropout_rate, l1=None, l2=None):\n",
    "    def make(inputs):\n",
    "        x = layers.Dense(units)(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(activation)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        return x\n",
    "    return make\n",
    "\n",
    "\n",
    "with strategy.scope():\n",
    "    # Wide Network\n",
    "    wide = keras.experimental.LinearModel()\n",
    "\n",
    "    # Deep Network\n",
    "    inputs = keras.Input(shape=[28])\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    deep = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Wide and Deep Network\n",
    "    wide_and_deep = keras.experimental.WideDeepModel(\n",
    "        linear_model=wide,\n",
    "        dnn_model=deep,\n",
    "        activation='sigmoid',\n",
    "    )\n",
    "\n",
    "wide_and_deep.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['AUC', 'binary_accuracy'],\n",
    "    experimental_steps_per_execution=steps_per_execution,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training #\n",
    "\n",
    "During training, we'll use the `EarlyStopping` callback as usual. Notice that we've also defined a **learning rate schedule**. It's been found that gradually decreasing the learning rate over the course of training can improve performance (the weights \"settle in\" to a minimum). This schedule will multiply the learning rate by `0.2` if the validation loss didn't decrease after an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(\n",
    "    patience=2,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "lr_schedule = callbacks.ReduceLROnPlateau(\n",
    "    patience=0,\n",
    "    factor=0.2,\n",
    "    min_lr=0.001,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = wide_and_deep.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping, lr_schedule],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\n",
    "history_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}