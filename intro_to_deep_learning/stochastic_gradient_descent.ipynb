{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In this exercise you'll train a neural network on the *Fuel Economy* dataset and then explore the effect of the learning rate and batch size on SGD.\n",
    "\n",
    "Setting up plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup plotting\n",
    "from learntools.deep_learning_intro.ex3 import *\n",
    "from learntools.core import binder\n",
    "import matplotlib.pyplot as plt\n",
    "from learntools.deep_learning_intro.dltools import animate_sgd\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('animation', html='html5')\n",
    "\n",
    "# Setup feedback system\n",
    "binder.bind(globals())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *Fuel Economy* dataset your task is to predict the fuel economy of an automobile given features like its type of engine or the year it was made. \n",
    "\n",
    "First load the dataset by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: [50]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fuel = pd.read_csv(\n",
    "    '~/Desktop/Kaggle/practice/Intro_to_Deep_Learning/fuel.csv')\n",
    "\n",
    "X = fuel.copy()\n",
    "# Remove target\n",
    "y = X.pop('FE')\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(),\n",
    "     make_column_selector(dtype_include=np.number)),\n",
    "    (OneHotEncoder(sparse=False),\n",
    "     make_column_selector(dtype_include=object)),\n",
    ")\n",
    "\n",
    "X = preprocessor.fit_transform(X)\n",
    "y = np.log(y)  # log transform target instead of standardizing\n",
    "\n",
    "input_shape = [X.shape[1]]\n",
    "print(\"Input shape: {}\".format(input_shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data if you like. Our target in this case is the `'FE'` column and the remaining columns are the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>40</th>\n      <th>41</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.913643</td>\n      <td>1.068005</td>\n      <td>0.524148</td>\n      <td>0.685653</td>\n      <td>-0.226455</td>\n      <td>0.391659</td>\n      <td>0.43492</td>\n      <td>0.463841</td>\n      <td>-0.447941</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.913643</td>\n      <td>1.068005</td>\n      <td>0.524148</td>\n      <td>0.685653</td>\n      <td>-0.226455</td>\n      <td>0.391659</td>\n      <td>0.43492</td>\n      <td>0.463841</td>\n      <td>-0.447941</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.530594</td>\n      <td>1.068005</td>\n      <td>0.524148</td>\n      <td>0.685653</td>\n      <td>-0.226455</td>\n      <td>0.391659</td>\n      <td>0.43492</td>\n      <td>0.463841</td>\n      <td>-0.447941</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.530594</td>\n      <td>1.068005</td>\n      <td>0.524148</td>\n      <td>0.685653</td>\n      <td>-0.226455</td>\n      <td>0.391659</td>\n      <td>0.43492</td>\n      <td>0.463841</td>\n      <td>-0.447941</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.296693</td>\n      <td>2.120794</td>\n      <td>0.524148</td>\n      <td>-1.458464</td>\n      <td>-0.226455</td>\n      <td>0.391659</td>\n      <td>0.43492</td>\n      <td>0.463841</td>\n      <td>-0.447941</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 50 columns</p>\n</div>",
      "text/plain": "         0         1         2         3         4         5        6   \\\n0  0.913643  1.068005  0.524148  0.685653 -0.226455  0.391659  0.43492   \n1  0.913643  1.068005  0.524148  0.685653 -0.226455  0.391659  0.43492   \n2  0.530594  1.068005  0.524148  0.685653 -0.226455  0.391659  0.43492   \n3  0.530594  1.068005  0.524148  0.685653 -0.226455  0.391659  0.43492   \n4  1.296693  2.120794  0.524148 -1.458464 -0.226455  0.391659  0.43492   \n\n         7         8    9   ...   40   41   42   43   44   45   46   47   48  \\\n0  0.463841 -0.447941  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1  0.463841 -0.447941  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2  0.463841 -0.447941  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3  0.463841 -0.447941  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4  0.463841 -0.447941  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n    49  \n0  0.0  \n1  0.0  \n2  0.0  \n3  0.0  \n4  0.0  \n\n[5 rows x 50 columns]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment to see original data\n",
    "fuel.head()\n",
    "# Uncomment to see processed features\n",
    "pd.DataFrame(X[:10, :]).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to define the network we'll use for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Add Loss and Optimizer\n",
    "\n",
    "Before training the network we need to define the loss and optimizer we'll use. Using the model's `compile` method, add the Adam optimizer and MAE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Train Model\n",
    "\n",
    "Once you've defined the model and compiled it with a loss and optimizer you're ready for training. Train the network for 200 epochs with a batch size of 128. The input data is `X` with target `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 3.2757\n",
      "Epoch 2/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.5969\n",
      "Epoch 3/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.8166\n",
      "Epoch 4/200\n",
      "9/9 [==============================] - 0s 935us/step - loss: 0.4479\n",
      "Epoch 5/200\n",
      "9/9 [==============================] - 0s 994us/step - loss: 0.3077\n",
      "Epoch 6/200\n",
      "9/9 [==============================] - 0s 990us/step - loss: 0.2485\n",
      "Epoch 7/200\n",
      "9/9 [==============================] - 0s 915us/step - loss: 0.1782\n",
      "Epoch 8/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.1522\n",
      "Epoch 9/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.1246\n",
      "Epoch 10/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.1075\n",
      "Epoch 11/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0954\n",
      "Epoch 12/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0837\n",
      "Epoch 13/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0801\n",
      "Epoch 14/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0685\n",
      "Epoch 15/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0618\n",
      "Epoch 16/200\n",
      "9/9 [==============================] - 0s 993us/step - loss: 0.0636\n",
      "Epoch 17/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0597\n",
      "Epoch 18/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0597\n",
      "Epoch 19/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0590\n",
      "Epoch 20/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0514\n",
      "Epoch 21/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0509\n",
      "Epoch 22/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0499\n",
      "Epoch 23/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0482\n",
      "Epoch 24/200\n",
      "9/9 [==============================] - 0s 983us/step - loss: 0.0455\n",
      "Epoch 25/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0469\n",
      "Epoch 26/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0502\n",
      "Epoch 27/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0455\n",
      "Epoch 28/200\n",
      "9/9 [==============================] - 0s 964us/step - loss: 0.0472\n",
      "Epoch 29/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0404\n",
      "Epoch 30/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0502\n",
      "Epoch 31/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0599\n",
      "Epoch 32/200\n",
      "9/9 [==============================] - 0s 979us/step - loss: 0.0463\n",
      "Epoch 33/200\n",
      "9/9 [==============================] - 0s 990us/step - loss: 0.0447\n",
      "Epoch 34/200\n",
      "9/9 [==============================] - 0s 964us/step - loss: 0.0425\n",
      "Epoch 35/200\n",
      "9/9 [==============================] - 0s 988us/step - loss: 0.0427\n",
      "Epoch 36/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0394\n",
      "Epoch 37/200\n",
      "9/9 [==============================] - 0s 978us/step - loss: 0.0349\n",
      "Epoch 38/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0407\n",
      "Epoch 39/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0367\n",
      "Epoch 40/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0393\n",
      "Epoch 41/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0384\n",
      "Epoch 42/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0365\n",
      "Epoch 43/200\n",
      "9/9 [==============================] - 0s 964us/step - loss: 0.0355\n",
      "Epoch 44/200\n",
      "9/9 [==============================] - 0s 940us/step - loss: 0.0368\n",
      "Epoch 45/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0512\n",
      "Epoch 46/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0453\n",
      "Epoch 47/200\n",
      "9/9 [==============================] - 0s 975us/step - loss: 0.0355\n",
      "Epoch 48/200\n",
      "9/9 [==============================] - 0s 941us/step - loss: 0.0321\n",
      "Epoch 49/200\n",
      "9/9 [==============================] - 0s 949us/step - loss: 0.0356\n",
      "Epoch 50/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0316\n",
      "Epoch 51/200\n",
      "9/9 [==============================] - 0s 927us/step - loss: 0.0348\n",
      "Epoch 52/200\n",
      "9/9 [==============================] - 0s 940us/step - loss: 0.0451\n",
      "Epoch 53/200\n",
      "9/9 [==============================] - 0s 964us/step - loss: 0.0460\n",
      "Epoch 54/200\n",
      "9/9 [==============================] - 0s 914us/step - loss: 0.0340\n",
      "Epoch 55/200\n",
      "9/9 [==============================] - 0s 985us/step - loss: 0.0341\n",
      "Epoch 56/200\n",
      "9/9 [==============================] - 0s 982us/step - loss: 0.0340\n",
      "Epoch 57/200\n",
      "9/9 [==============================] - 0s 986us/step - loss: 0.0310\n",
      "Epoch 58/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0375\n",
      "Epoch 59/200\n",
      "9/9 [==============================] - 0s 936us/step - loss: 0.0429\n",
      "Epoch 60/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0416\n",
      "Epoch 61/200\n",
      "9/9 [==============================] - 0s 958us/step - loss: 0.0366\n",
      "Epoch 62/200\n",
      "9/9 [==============================] - 0s 983us/step - loss: 0.0357\n",
      "Epoch 63/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 64/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0369\n",
      "Epoch 65/200\n",
      "9/9 [==============================] - 0s 968us/step - loss: 0.0368\n",
      "Epoch 66/200\n",
      "9/9 [==============================] - 0s 955us/step - loss: 0.0342\n",
      "Epoch 67/200\n",
      "9/9 [==============================] - 0s 920us/step - loss: 0.0329\n",
      "Epoch 68/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0325\n",
      "Epoch 69/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0329\n",
      "Epoch 70/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0380\n",
      "Epoch 71/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0445\n",
      "Epoch 72/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0432\n",
      "Epoch 73/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0344\n",
      "Epoch 74/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0319\n",
      "Epoch 75/200\n",
      "9/9 [==============================] - 0s 947us/step - loss: 0.0345\n",
      "Epoch 76/200\n",
      "9/9 [==============================] - 0s 927us/step - loss: 0.0318\n",
      "Epoch 77/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0310\n",
      "Epoch 78/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0311\n",
      "Epoch 79/200\n",
      "9/9 [==============================] - 0s 991us/step - loss: 0.0304\n",
      "Epoch 80/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0294\n",
      "Epoch 81/200\n",
      "9/9 [==============================] - 0s 967us/step - loss: 0.0326\n",
      "Epoch 82/200\n",
      "9/9 [==============================] - 0s 957us/step - loss: 0.0322\n",
      "Epoch 83/200\n",
      "9/9 [==============================] - 0s 924us/step - loss: 0.0313\n",
      "Epoch 84/200\n",
      "9/9 [==============================] - 0s 967us/step - loss: 0.0346\n",
      "Epoch 85/200\n",
      "9/9 [==============================] - 0s 977us/step - loss: 0.0380\n",
      "Epoch 86/200\n",
      "9/9 [==============================] - 0s 982us/step - loss: 0.0300\n",
      "Epoch 87/200\n",
      "9/9 [==============================] - 0s 998us/step - loss: 0.0290\n",
      "Epoch 88/200\n",
      "9/9 [==============================] - 0s 895us/step - loss: 0.0277\n",
      "Epoch 89/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0324\n",
      "Epoch 90/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0330\n",
      "Epoch 91/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0348\n",
      "Epoch 92/200\n",
      "9/9 [==============================] - 0s 924us/step - loss: 0.0300\n",
      "Epoch 93/200\n",
      "9/9 [==============================] - 0s 949us/step - loss: 0.0325\n",
      "Epoch 94/200\n",
      "9/9 [==============================] - 0s 960us/step - loss: 0.0380\n",
      "Epoch 95/200\n",
      "9/9 [==============================] - 0s 942us/step - loss: 0.0367\n",
      "Epoch 96/200\n",
      "9/9 [==============================] - 0s 991us/step - loss: 0.0399\n",
      "Epoch 97/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0310\n",
      "Epoch 98/200\n",
      "9/9 [==============================] - 0s 931us/step - loss: 0.0327\n",
      "Epoch 99/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0281\n",
      "Epoch 100/200\n",
      "9/9 [==============================] - 0s 998us/step - loss: 0.0317\n",
      "Epoch 101/200\n",
      "9/9 [==============================] - 0s 987us/step - loss: 0.0383\n",
      "Epoch 102/200\n",
      "9/9 [==============================] - 0s 966us/step - loss: 0.0359\n",
      "Epoch 103/200\n",
      "9/9 [==============================] - 0s 913us/step - loss: 0.0412\n",
      "Epoch 104/200\n",
      "9/9 [==============================] - 0s 958us/step - loss: 0.0399\n",
      "Epoch 105/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0376\n",
      "Epoch 106/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0428\n",
      "Epoch 107/200\n",
      "9/9 [==============================] - 0s 994us/step - loss: 0.0540\n",
      "Epoch 108/200\n",
      "9/9 [==============================] - 0s 924us/step - loss: 0.0464\n",
      "Epoch 109/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0344\n",
      "Epoch 110/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0309\n",
      "Epoch 111/200\n",
      "9/9 [==============================] - 0s 980us/step - loss: 0.0327\n",
      "Epoch 112/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0277\n",
      "Epoch 113/200\n",
      "9/9 [==============================] - 0s 946us/step - loss: 0.0285\n",
      "Epoch 114/200\n",
      "9/9 [==============================] - 0s 976us/step - loss: 0.0272\n",
      "Epoch 115/200\n",
      "9/9 [==============================] - 0s 964us/step - loss: 0.0304\n",
      "Epoch 116/200\n",
      "9/9 [==============================] - 0s 964us/step - loss: 0.0359\n",
      "Epoch 117/200\n",
      "9/9 [==============================] - 0s 935us/step - loss: 0.0295\n",
      "Epoch 118/200\n",
      "9/9 [==============================] - 0s 947us/step - loss: 0.0301\n",
      "Epoch 119/200\n",
      "9/9 [==============================] - 0s 988us/step - loss: 0.0294\n",
      "Epoch 120/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0312\n",
      "Epoch 121/200\n",
      "9/9 [==============================] - 0s 980us/step - loss: 0.0283\n",
      "Epoch 122/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0274\n",
      "Epoch 123/200\n",
      "9/9 [==============================] - 0s 894us/step - loss: 0.0321\n",
      "Epoch 124/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0335\n",
      "Epoch 125/200\n",
      "9/9 [==============================] - 0s 966us/step - loss: 0.0331\n",
      "Epoch 126/200\n",
      "9/9 [==============================] - 0s 976us/step - loss: 0.0314\n",
      "Epoch 127/200\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0281\n",
      "Epoch 128/200\n",
      "9/9 [==============================] - 0s 998us/step - loss: 0.0305\n",
      "Epoch 129/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0323\n",
      "Epoch 130/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0273\n",
      "Epoch 131/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0259\n",
      "Epoch 132/200\n",
      "9/9 [==============================] - 0s 971us/step - loss: 0.0251\n",
      "Epoch 133/200\n",
      "9/9 [==============================] - 0s 926us/step - loss: 0.0263\n",
      "Epoch 134/200\n",
      "9/9 [==============================] - 0s 957us/step - loss: 0.0315\n",
      "Epoch 135/200\n",
      "9/9 [==============================] - 0s 942us/step - loss: 0.0299\n",
      "Epoch 136/200\n",
      "9/9 [==============================] - 0s 929us/step - loss: 0.0309\n",
      "Epoch 137/200\n",
      "9/9 [==============================] - 0s 972us/step - loss: 0.0275\n",
      "Epoch 138/200\n",
      "9/9 [==============================] - 0s 940us/step - loss: 0.0268\n",
      "Epoch 139/200\n",
      "9/9 [==============================] - 0s 949us/step - loss: 0.0307\n",
      "Epoch 140/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0279\n",
      "Epoch 141/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0271\n",
      "Epoch 142/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0341\n",
      "Epoch 143/200\n",
      "9/9 [==============================] - 0s 984us/step - loss: 0.0340\n",
      "Epoch 144/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0286\n",
      "Epoch 145/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0302\n",
      "Epoch 146/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0343\n",
      "Epoch 147/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0311\n",
      "Epoch 148/200\n",
      "9/9 [==============================] - 0s 911us/step - loss: 0.0307\n",
      "Epoch 149/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0422\n",
      "Epoch 150/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0299\n",
      "Epoch 151/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0354\n",
      "Epoch 152/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0252\n",
      "Epoch 153/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0276\n",
      "Epoch 154/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0266\n",
      "Epoch 155/200\n",
      "9/9 [==============================] - 0s 994us/step - loss: 0.0273\n",
      "Epoch 156/200\n",
      "9/9 [==============================] - 0s 996us/step - loss: 0.0325\n",
      "Epoch 157/200\n",
      "9/9 [==============================] - 0s 991us/step - loss: 0.0364\n",
      "Epoch 158/200\n",
      "9/9 [==============================] - 0s 987us/step - loss: 0.0307\n",
      "Epoch 159/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0321\n",
      "Epoch 160/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0299\n",
      "Epoch 161/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0261\n",
      "Epoch 162/200\n",
      "9/9 [==============================] - 0s 969us/step - loss: 0.0243\n",
      "Epoch 163/200\n",
      "9/9 [==============================] - 0s 978us/step - loss: 0.0306\n",
      "Epoch 164/200\n",
      "9/9 [==============================] - 0s 948us/step - loss: 0.0253\n",
      "Epoch 165/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0253\n",
      "Epoch 166/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0373\n",
      "Epoch 167/200\n",
      "9/9 [==============================] - 0s 930us/step - loss: 0.0472\n",
      "Epoch 168/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0362\n",
      "Epoch 169/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0289\n",
      "Epoch 170/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0305\n",
      "Epoch 171/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0338\n",
      "Epoch 172/200\n",
      "9/9 [==============================] - 0s 955us/step - loss: 0.0293\n",
      "Epoch 173/200\n",
      "9/9 [==============================] - 0s 983us/step - loss: 0.0274\n",
      "Epoch 174/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0301\n",
      "Epoch 175/200\n",
      "9/9 [==============================] - 0s 934us/step - loss: 0.0277\n",
      "Epoch 176/200\n",
      "9/9 [==============================] - 0s 978us/step - loss: 0.0273\n",
      "Epoch 177/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0257\n",
      "Epoch 178/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0290\n",
      "Epoch 179/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0278\n",
      "Epoch 180/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0247\n",
      "Epoch 181/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0255\n",
      "Epoch 182/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0290\n",
      "Epoch 183/200\n",
      "9/9 [==============================] - 0s 967us/step - loss: 0.0259\n",
      "Epoch 184/200\n",
      "9/9 [==============================] - 0s 990us/step - loss: 0.0255\n",
      "Epoch 185/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0306\n",
      "Epoch 186/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0243\n",
      "Epoch 187/200\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0256\n",
      "Epoch 188/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0268\n",
      "Epoch 189/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0288\n",
      "Epoch 190/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0300\n",
      "Epoch 191/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0271\n",
      "Epoch 192/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0260\n",
      "Epoch 193/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0252\n",
      "Epoch 194/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0267\n",
      "Epoch 195/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0295\n",
      "Epoch 196/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0280\n",
      "Epoch 197/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0274\n",
      "Epoch 198/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0292\n",
      "Epoch 199/200\n",
      "9/9 [==============================] - 0s 976us/step - loss: 0.0305\n",
      "Epoch 200/200\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0346\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X, y,\n",
    "    batch_size=128,\n",
    "    epochs=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to look at the loss curves and evaluate the training. Run the cell below to get a plot of the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5. You can change this to get a different view.\n",
    "history_df.loc[5:, ['loss']].plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Evaluate Training\n",
    "\n",
    "If you trained the model longer, would you expect the loss to decrease further?\n",
    "\n",
    "* This depends on how the loss has evolved during training: if the learning curves have levelled off, there won't usually be any advantage to training for additional epochs. Conversely, if the loss appears to still be decreasing, then training for longer could be advantageous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the learning rate and the batch size, you have some control over:\n",
    "- How long it takes to train a model\n",
    "- How noisy the learning curves are\n",
    "- How small the loss becomes\n",
    "\n",
    "To get a better understanding of these two parameters, we'll look at the linear model, our ppsimplest neural network. Having only a single weight and a bias, it's easier to see what effect a change of parameter has.\n",
    "\n",
    "The next cell will generate an animation like the one in the tutorial. Change the values for `learning_rate`, `batch_size`, and `num_examples` (how many data points) and then run the cell. (It may take a moment or two.) Try the following combinations, or try some of your own:\n",
    "\n",
    "| `learning_rate` | `batch_size` | `num_examples` |\n",
    "|-----------------|--------------|----------------|\n",
    "| 0.05            | 32           | 256            |\n",
    "| 0.05            | 2            | 256            |\n",
    "| 0.05            | 128          | 256            |\n",
    "| 0.02            | 32           | 256            |\n",
    "| 0.2             | 32           | 256            |\n",
    "| 1.0             | 32           | 256            |\n",
    "| 0.9             | 4096         | 8192           |\n",
    "| 0.99            | 4096         | 8192           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Experiment with different values for the learning rate, batch size, and number of examples\n",
    "learning_rate = 0.05\n",
    "batch_size = 32\n",
    "num_examples = 256\n",
    "\n",
    "animate_sgd(\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    "    num_examples=num_examples,\n",
    "    # You can also change these, if you like\n",
    "    steps=50,  # total training steps (batches seen)\n",
    "    true_w=3.0,  # the slope of the data\n",
    "    true_b=2.0,  # the bias of the data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Learning Rate and Batch Size\n",
    "\n",
    "What effect did changing these parameters have? After you've thought about it, run the cell below for some discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You probably saw that smaller batch sizes gave noisier weight updates and loss curves. This is because each batch is a small sample of data and smaller samples tend to give noisier estimates. Smaller batches can have an \"averaging\" effect though which can be beneficial.\n",
    "\n",
    "* Smaller learning rates make the updates smaller and the training takes longer to converge. Large learning rates can speed up training, but don't \"settle in\" to a minimum as well. When the learning rate is too large, the training can fail completely. (Try setting the learning rate to a large value like 0.99 to see this.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('env')",
   "metadata": {
    "interpreter": {
     "hash": "5a12186ad70c36137f2ef09680302bb20e630f58e2e6fe7a1aa6e1503968199f"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}